{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Volumes/DATA/project1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMTokenizerFast\n",
    "from src.data_utils.funsd_procesor import preprocess_funsd_for_layoutlm\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'boxes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Correct the key name here from 'boxes' to 'bboxes'\u001b[39;00m\n\u001b[32m      7\u001b[39m sample = {\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m: sample_data[\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;66;03m# Also corrected 'tokens' to 'words' to match the function's expectation\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbboxes\u001b[39m\u001b[33m'\u001b[39m: sample_data[\u001b[33m'\u001b[39m\u001b[33mbboxes\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mner_tags\u001b[39m\u001b[33m'\u001b[39m: sample_data[\u001b[33m'\u001b[39m\u001b[33mner_tags\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m encodings = preprocess_funsd_for_layoutlm(sample, tokenizer, max_seq_length=\u001b[32m128\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m({k: \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m encodings.items()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/DATA/project1/src/data_utils/funsd_procesor.py:35\u001b[39m, in \u001b[36mpreprocess_funsd_for_layoutlm\u001b[39m\u001b[34m(examples, tokenizer, max_seq_length)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_funsd_for_layoutlm\u001b[39m(examples, tokenizer: LayoutLMTokenizerFast, max_seq_length=\u001b[32m512\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     tokenized = tokenizer(\n\u001b[32m     36\u001b[39m         examples[\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m],  \u001b[38;5;66;03m# <--- Change 'tokens' to 'words' here\u001b[39;00m\n\u001b[32m     37\u001b[39m         boxes=examples[\u001b[33m'\u001b[39m\u001b[33mbboxes\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     38\u001b[39m         padding=\u001b[33m'\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     39\u001b[39m         truncation=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     40\u001b[39m         max_length=max_seq_length,\n\u001b[32m     41\u001b[39m         return_offsets_mapping=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     42\u001b[39m         return_length=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     43\u001b[39m     )\n\u001b[32m     44\u001b[39m     processed = {\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: []}\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# The rest of your function remains the same...\u001b[39;00m\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Make sure the logic below correctly handles the tokenized output\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# (e.g., aligning labels with the tokenized words)\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# Example placeholder for the label alignment logic (adjust as needed)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/DATA/project1/ndlinear_docai/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28mself\u001b[39m._call_one(text=text, text_pair=text_pair, **all_kwargs)\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/DATA/project1/ndlinear_docai/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2975\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m         )\n\u001b[32m   2974\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2976\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2977\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   2978\u001b[39m         padding=padding,\n\u001b[32m   2979\u001b[39m         truncation=truncation,\n\u001b[32m   2980\u001b[39m         max_length=max_length,\n\u001b[32m   2981\u001b[39m         stride=stride,\n\u001b[32m   2982\u001b[39m         is_split_into_words=is_split_into_words,\n\u001b[32m   2983\u001b[39m         pad_to_multiple_of=pad_to_multiple_of,\n\u001b[32m   2984\u001b[39m         padding_side=padding_side,\n\u001b[32m   2985\u001b[39m         return_tensors=return_tensors,\n\u001b[32m   2986\u001b[39m         return_token_type_ids=return_token_type_ids,\n\u001b[32m   2987\u001b[39m         return_attention_mask=return_attention_mask,\n\u001b[32m   2988\u001b[39m         return_overflowing_tokens=return_overflowing_tokens,\n\u001b[32m   2989\u001b[39m         return_special_tokens_mask=return_special_tokens_mask,\n\u001b[32m   2990\u001b[39m         return_offsets_mapping=return_offsets_mapping,\n\u001b[32m   2991\u001b[39m         return_length=return_length,\n\u001b[32m   2992\u001b[39m         verbose=verbose,\n\u001b[32m   2993\u001b[39m         split_special_tokens=split_special_tokens,\n\u001b[32m   2994\u001b[39m         **kwargs,\n\u001b[32m   2995\u001b[39m     )\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   2998\u001b[39m         text=text,\n\u001b[32m   2999\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3017\u001b[39m         **kwargs,\n\u001b[32m   3018\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/DATA/project1/ndlinear_docai/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3177\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3167\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3168\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3169\u001b[39m     padding=padding,\n\u001b[32m   3170\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3174\u001b[39m     **kwargs,\n\u001b[32m   3175\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._batch_encode_plus(\n\u001b[32m   3178\u001b[39m     batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3179\u001b[39m     add_special_tokens=add_special_tokens,\n\u001b[32m   3180\u001b[39m     padding_strategy=padding_strategy,\n\u001b[32m   3181\u001b[39m     truncation_strategy=truncation_strategy,\n\u001b[32m   3182\u001b[39m     max_length=max_length,\n\u001b[32m   3183\u001b[39m     stride=stride,\n\u001b[32m   3184\u001b[39m     is_split_into_words=is_split_into_words,\n\u001b[32m   3185\u001b[39m     pad_to_multiple_of=pad_to_multiple_of,\n\u001b[32m   3186\u001b[39m     padding_side=padding_side,\n\u001b[32m   3187\u001b[39m     return_tensors=return_tensors,\n\u001b[32m   3188\u001b[39m     return_token_type_ids=return_token_type_ids,\n\u001b[32m   3189\u001b[39m     return_attention_mask=return_attention_mask,\n\u001b[32m   3190\u001b[39m     return_overflowing_tokens=return_overflowing_tokens,\n\u001b[32m   3191\u001b[39m     return_special_tokens_mask=return_special_tokens_mask,\n\u001b[32m   3192\u001b[39m     return_offsets_mapping=return_offsets_mapping,\n\u001b[32m   3193\u001b[39m     return_length=return_length,\n\u001b[32m   3194\u001b[39m     verbose=verbose,\n\u001b[32m   3195\u001b[39m     split_special_tokens=split_special_tokens,\n\u001b[32m   3196\u001b[39m     **kwargs,\n\u001b[32m   3197\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'boxes'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = '/Volumes/DATA/project1/data/raw/funsd'\n",
    "dataset = load_from_disk(data_dir)\n",
    "from transformers import LayoutLMTokenizerFast\n",
    "tokenizer = LayoutLMTokenizerFast.from_pretrained('microsoft/layoutlm-base-uncased')\n",
    "sample_data = dataset['train'][:2] # Renamed to avoid overwriting 'sample' variable\n",
    "\n",
    "# Correct the key name here from 'boxes' to 'bboxes'\n",
    "sample = {\n",
    "    'words': sample_data['words'], # Also corrected 'tokens' to 'words' to match the function's expectation\n",
    "    'bboxes': sample_data['bboxes'],\n",
    "    'ner_tags': sample_data['ner_tags']\n",
    "}\n",
    "encodings = preprocess_funsd_for_layoutlm(sample, tokenizer, max_seq_length=128)\n",
    "print(type(tokenizer))\n",
    "print({k: len(v) for k, v in encodings.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokens = tokenizer.convert_ids_to_tokens(encodings[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m      2\u001b[39m bboxes = encodings[\u001b[33m'\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      3\u001b[39m labels = encodings[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'encodings' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'][0])\n",
    "bboxes = encodings['bbox'][0]\n",
    "labels = encodings['labels'][0]\n",
    "for tok, box, lbl in zip(tokens[:20], bboxes[:20], labels[:20]):\n",
    "    print(f\"Token: {tok:10} | Box: {box} | Label: {lbl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
